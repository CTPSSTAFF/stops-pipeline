{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb21a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "\n",
    "class StopsPRNExtractor:\n",
    "    def __init__(self):\n",
    "        self.files = {}  # {alias: file_path}\n",
    "        self.tables = {} # {alias: DataFrame}\n",
    "        self.metadata = {} # {alias: metadata dict}\n",
    "\n",
    "    def add_files(self, file_info_list):\n",
    "        \"\"\"\n",
    "        Add multiple files to the extractor.\n",
    "        file_info_list: list of tuples [(alias, file_path), ...]\n",
    "        \"\"\"\n",
    "        for alias, file_path in file_info_list:\n",
    "            self.files[alias] = file_path\n",
    "\n",
    "    def extract_table_10_01(self):\n",
    "        \"\"\"\n",
    "        Extract Table 10.01 and metadata from all loaded files.\n",
    "        Populates self.tables and self.metadata.\n",
    "        \"\"\"\n",
    "        for alias, file_path in self.files.items():\n",
    "            df, meta = self._extract_table_10_01_from_prn(file_path)\n",
    "            if not df.empty:\n",
    "                # For Table 10.01, store directly under the alias\n",
    "                self.tables[alias] = df\n",
    "                self.metadata[alias] = meta\n",
    "            else:\n",
    "                print(f\"Warning: Table 10.01 could not be extracted from {file_path}\")\n",
    "\n",
    "    def extract_table_9_01(self):\n",
    "        \"\"\"\n",
    "        Extract Table 9.01 and metadata from all loaded files.\n",
    "        Populates self.tables and self.metadata.\n",
    "        \"\"\"\n",
    "        for alias, file_path in self.files.items():\n",
    "            df, meta = self._extract_table_9_01_from_prn(file_path)\n",
    "            if not df.empty:\n",
    "                # For Table 9.01, store under alias_table_9_01\n",
    "                self.tables[f\"{alias}_table_9_01\"] = df\n",
    "                self.metadata[f\"{alias}_table_9_01\"] = meta\n",
    "            else:\n",
    "                print(f\"Warning: Table 9.01 could not be extracted from {file_path}\")\n",
    "\n",
    "    def export_to_csv(self, output_dir=\"extracted_tables\"):\n",
    "        \"\"\"\n",
    "        Exports all extracted tables to individual CSV files.\n",
    "        Each file will be named according to its extracted alias and table type.\n",
    "        output_dir: directory where CSV files will be saved.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created directory: {output_dir}\")\n",
    "\n",
    "        for alias_key, dataframe in self.tables.items():\n",
    "            file_name = \"\"\n",
    "            # Check if it's a Table 9.01 entry (indicated by the suffix in the key)\n",
    "            if \"_table_9_01\" in alias_key:\n",
    "                original_alias = alias_key.replace(\"_table_9_01\", \"\")\n",
    "                file_name = f\"{original_alias}_Table_9_01.csv\"\n",
    "            else:\n",
    "                # Assume it's a Table 10.01 entry (or other default extraction)\n",
    "                # The alias_key itself is the original alias for Table 10.01\n",
    "                file_name = f\"{alias_key}_Table_10_01.csv\"\n",
    "            \n",
    "            output_path = os.path.join(output_dir, file_name)\n",
    "            try:\n",
    "                dataframe.to_csv(output_path, index=False)\n",
    "                print(f\"Exported '{alias_key}' to '{output_path}' successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error exporting '{alias_key}' to CSV: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_metadata_from_prn(lines, start_index):\n",
    "        metadata = {}\n",
    "        for meta_line_offset in range(1, 10):\n",
    "            meta_line_num = start_index - meta_line_offset\n",
    "            if meta_line_num >= 0:\n",
    "                meta_line = lines[meta_line_num].strip()\n",
    "                if \"Program STOPS\" in meta_line:\n",
    "                    program_version_parts = meta_line.split(\" - \", 1)\n",
    "                    if len(program_version_parts) > 0:\n",
    "                        metadata[\"Program\"] = program_version_parts[0].replace(\"Program \", \"\").strip()\n",
    "                    if len(program_version_parts) > 1 and \"Version:\" in program_version_parts[1]:\n",
    "                        version_match = re.search(r'Version:\\s*(\\S+)\\s*-\\s*(\\d{2}/\\d{2}/\\d{4})', program_version_parts[1])\n",
    "                        if version_match:\n",
    "                            metadata[\"Version\"] = f\"{version_match.group(1)} - {version_match.group(2)}\"\n",
    "                        else:\n",
    "                            metadata[\"Version\"] = program_version_parts[1].split(\"Version: \")[1].split(\" - \")[0].strip()\n",
    "                    elif \"Version:\" in meta_line:\n",
    "                        version_match = re.search(r'Version:\\s*(\\S+)\\s*-\\s*(\\d{2}/\\d{2}/\\d{4})', meta_line)\n",
    "                        if version_match:\n",
    "                            metadata[\"Version\"] = f\"{version_match.group(1)} - {version_match.group(2)}\"\n",
    "                elif \"Run:\" in meta_line:\n",
    "                    parts = meta_line.split(\"Run:\")\n",
    "                    if len(parts) > 1:\n",
    "                        run_system_part = parts[1].strip()\n",
    "                        run_match = re.search(r'^(.*?)(?:\\s+System:\\s*(.*))?$', run_system_part)\n",
    "                        if run_match:\n",
    "                            metadata[\"Run\"] = run_match.group(1).strip()\n",
    "                            if run_match.group(2):\n",
    "                                metadata[\"System\"] = run_match.group(2).strip()\n",
    "                        else:\n",
    "                            metadata[\"Run\"] = run_system_part\n",
    "                elif \"Page\" in meta_line:\n",
    "                    page_match = re.search(r'Page\\s+(\\d+)', meta_line)\n",
    "                    if page_match:\n",
    "                        metadata[\"Page\"] = page_match.group(1).strip()\n",
    "        return metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_table_10_01_from_prn(file_path):\n",
    "        metadata = {}\n",
    "        actual_data_lines = []\n",
    "        in_table_10_01_section = False\n",
    "        started_collecting_data = False\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "        except FileNotFoundError:\n",
    "            return pd.DataFrame(), {}\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.search(r\"Table\\s+10\\.01\", line):\n",
    "                in_table_10_01_section = True\n",
    "                metadata = StopsPRNExtractor._extract_metadata_from_prn(lines, i)\n",
    "                continue\n",
    "\n",
    "            if in_table_10_01_section:\n",
    "                if i + 1 < len(lines):\n",
    "                    header_line_check = lines[i].strip()\n",
    "                    if \"Route_ID\" in header_line_check and \"Route Name\" in header_line_check and \\\n",
    "                       \"Count\" in header_line_check and \"ALL\" in header_line_check and \\\n",
    "                       re.search(r\"^=+\\s+=+\\s+=+.*\", lines[i+1]):\n",
    "                        started_collecting_data = True\n",
    "                        continue \n",
    "\n",
    "                if started_collecting_data:\n",
    "                    if \"Total\" in line and re.search(r\"={20,}\", lines[i+1] if i + 1 < len(lines) else \"\"):\n",
    "                        actual_data_lines.append(line.rstrip()) \n",
    "                        in_table_10_01_section = False\n",
    "                        break\n",
    "\n",
    "                    if re.search(r\"Table\\s+\\d+\\.\\d+\", line) or (line.strip() and \"Program STOPS\" in line):\n",
    "                        in_table_10_01_section = False\n",
    "                        break\n",
    "                    \n",
    "                    if line.strip() and not re.fullmatch(r\"={2,}\", line.strip()) and not re.fullmatch(r\"-{2,}\", line.strip()):\n",
    "                        actual_data_lines.append(line.rstrip())\n",
    "\n",
    "        if not actual_data_lines:\n",
    "            return pd.DataFrame(), metadata\n",
    "\n",
    "        colspecs = [\n",
    "            (0, 20),   # Route_ID\n",
    "            (20, 56),  # --Route Name\n",
    "            (56, 65),  # Count\n",
    "            (65, 75),  # Y2024_EXISTING_WLK\n",
    "            (75, 85),  # Y2024_EXISTING_KNR\n",
    "            (85, 95),  # Y2024_EXISTING_PNR\n",
    "            (95, 105), # Y2024_EXISTING_ALL\n",
    "            (105, 115),# Y2050_NO-BUILD_WLK\n",
    "            (115, 125),# Y2050_NO-BUILD_KNR\n",
    "            (125, 135),# Y2050_NO-BUILD_PNR\n",
    "            (135, 145),# Y2050_NO-BUILD_ALL\n",
    "            (145, 155),# Y2050_BUILD_WLK\n",
    "            (155, 165),# Y2050_BUILD_KNR\n",
    "            (165, 175),# Y2050_BUILD_PNR\n",
    "            (175, 185) # Y2050_BUILD_ALL\n",
    "        ]\n",
    "\n",
    "        names = [\n",
    "            \"Route_ID\", \"Route_Name\",\n",
    "            \"Count\", \"Y2024_EXISTING_WLK\", \"Y2024_EXISTING_KNR\", \"Y2024_EXISTING_PNR\", \"Y2024_EXISTING_ALL\",\n",
    "            \"Y2050_NO-BUILD_WLK\", \"Y2050_NO-BUILD_KNR\", \"Y2050_NO-BUILD_PNR\", \"Y2050_NO-BUILD_ALL\",\n",
    "            \"Y2050_BUILD_WLK\", \"Y2050_BUILD_KNR\", \"Y2050_BUILD_PNR\", \"Y2050_BUILD_ALL\"\n",
    "        ]\n",
    "        \n",
    "        data_for_df = io.StringIO('\\n'.join(actual_data_lines))\n",
    "        df = pd.read_fwf(data_for_df, colspecs=colspecs, header=None, names=names,\n",
    "                         dtype={col: str for col in names})\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "            if col not in [\"Route_ID\", \"Route_Name\"]:\n",
    "                df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')\n",
    "                df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "        return df, metadata\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_table_9_01_from_prn(file_path):\n",
    "        metadata = {}\n",
    "        actual_data_lines = []\n",
    "        in_table_9_01_section = False\n",
    "        started_collecting_data = False\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "        except FileNotFoundError:\n",
    "            return pd.DataFrame(), {}\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if re.search(r\"Table\\s+9\\.01\", line):\n",
    "                in_table_9_01_section = True\n",
    "                metadata = StopsPRNExtractor._extract_metadata_from_prn(lines, i)\n",
    "                continue\n",
    "\n",
    "            if in_table_9_01_section:\n",
    "                if not started_collecting_data and i + 1 < len(lines):\n",
    "                    header_line_check = lines[i].strip()\n",
    "                    separator_line_check = lines[i+1].strip()\n",
    "\n",
    "                    header_pattern_match = re.search(r\"Stop_id1\\s+.*?Station Name\\s+.*?WLK\\s+.*?KNR\\s+.*?PNR\\s+.*?XFR\\s+.*?ALL\", header_line_check)\n",
    "                    separator_pattern_match = re.search(r\"^=+\\s+=+\\s+=+.*\", separator_line_check)\n",
    "\n",
    "                    if header_pattern_match and separator_pattern_match:\n",
    "                        started_collecting_data = True\n",
    "                        continue \n",
    "\n",
    "                if started_collecting_data:\n",
    "                    if \"Total\" in line and re.search(r\"={20,}\", lines[i+1] if i + 1 < len(lines) else \"\"):\n",
    "                        actual_data_lines.append(line.rstrip()) \n",
    "                        in_table_9_01_section = False\n",
    "                        break\n",
    "\n",
    "                    if re.search(r\"Table\\s+\\d+\\.\\d+\", line) or (line.strip() and \"Program STOPS\" in line):\n",
    "                        in_table_9_01_section = False\n",
    "                        break\n",
    "                    \n",
    "                    if line.strip() and not re.fullmatch(r\"={2,}\", line.strip()) and not re.fullmatch(r\"-{2,}\", line.strip()):\n",
    "                        actual_data_lines.append(line.rstrip())\n",
    "\n",
    "        if not actual_data_lines:\n",
    "            return pd.DataFrame(), metadata\n",
    "\n",
    "        colspecs = [\n",
    "            (0, 26),   # Stop_id1\n",
    "            (26, 47),  # Station Name\n",
    "            (47, 58),  # Y2024_EXISTING_WLK\n",
    "            (58, 68),  # Y2024_EXISTING_KNR\n",
    "            (68, 78),  # Y2024_EXISTING_PNR\n",
    "            (78, 88),  # Y2024_EXISTING_XFR\n",
    "            (88, 98),  # Y2024_EXISTING_ALL\n",
    "            (98, 109), # Y2050_NO-BUILD_WLK\n",
    "            (109, 119),# Y2050_NO-BUILD_KNR\n",
    "            (119, 129),# Y2050_NO-BUILD_PNR\n",
    "            (129, 139),# Y2050_NO-BUILD_XFR\n",
    "            (139, 149),# Y2050_NO-BUILD_ALL\n",
    "            (149, 160),# Y2050_BUILD_WLK\n",
    "            (160, 170),# Y2050_BUILD_KNR\n",
    "            (170, 180),# Y2050_BUILD_PNR\n",
    "            (180, 190),# Y2050_BUILD_XFR\n",
    "            (190, 200) # Y2050_BUILD_ALL\n",
    "        ]\n",
    "\n",
    "        names = [\n",
    "            \"Stop_id1\", \"Station_Name\",\n",
    "            \"Y2024_EXISTING_WLK\", \"Y2024_EXISTING_KNR\", \"Y2024_EXISTING_PNR\", \"Y2024_EXISTING_XFR\", \"Y2024_EXISTING_ALL\",\n",
    "            \"Y2050_NO-BUILD_WLK\", \"Y2050_NO-BUILD_KNR\", \"Y2050_NO-BUILD_PNR\", \"Y2050_NO-BUILD_XFR\", \"Y2050_NO-BUILD_ALL\",\n",
    "            \"Y2050_BUILD_WLK\", \"Y2050_BUILD_KNR\", \"Y2050_BUILD_PNR\", \"Y2050_BUILD_XFR\", \"Y2050_BUILD_ALL\"\n",
    "        ]\n",
    "        \n",
    "        data_for_df = io.StringIO('\\n'.join(actual_data_lines))\n",
    "        df = pd.read_fwf(data_for_df, colspecs=colspecs, header=None, names=names,\n",
    "                         dtype={col: str for col in names})\n",
    "\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "            if col not in [\"Stop_id1\", \"Station_Name\"]:\n",
    "                df[col] = pd.to_numeric(df[col].str.replace(',', ''), errors='coerce')\n",
    "                df[col] = df[col].fillna(0).astype(int)\n",
    "\n",
    "        return df, metadata\n",
    "\n",
    "\n",
    "# Instantiate the extractor object\n",
    "extractor = StopsPRNExtractor()\n",
    "\n",
    "prn_files_directory = \"./stops_output_prn_report_files\"\n",
    "\n",
    "prn_files = [\n",
    "    (\"2024\", f\"{prn_files_directory}/A2_MBTA-CATA-MWRTA-BATA-MVRTA#MBTA-CATA-MWRTA-BATA-MVRTA#MBTA-CATA-MWRTA-BATA-MVRTA_STOPSY2024Results.prn\"),\n",
    "    (\"2045\", f\"{prn_files_directory}/A2_MBTA-CATA-MWRTA-BATA-MVRTA#MBTA50-CATA-MWRTA-BATA-MVRTA#MBTA50-CATA-MWRTA-BATA-MVRTA_STOPSY2045Results.prn\"),\n",
    "    (\"2050\", f\"{prn_files_directory}/A2_MBTA-CATA-MWRTA-BATA-MVRTA#MBTA50-CATA-MWRTA-BATA-MVRTA#MBTA50-CATA-MWRTA-BATA-MVRTA_STOPSY2050Results.prn\"),\n",
    "    # Add more as needed\n",
    "]\n",
    "\n",
    "# Add files to the extractor\n",
    "extractor.add_files(prn_files)\n",
    "\n",
    "# Extract Table 10.01 from all loaded files\n",
    "extractor.extract_table_10_01() \n",
    "\n",
    "# Extract Table 9.01 from all loaded files\n",
    "extractor.extract_table_9_01()\n",
    "\n",
    "# Display results (optional, good for debugging)\n",
    "for alias_key in extractor.tables:\n",
    "    print(f\"\\nAlias: {alias_key}\")\n",
    "    print(\"Metadata:\")\n",
    "    for k, v in extractor.metadata[alias_key].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(f\"\\nTable data for {alias_key}:\")\n",
    "    display(extractor.tables[alias_key])\n",
    "\n",
    "# Export extracted tables to CSV files\n",
    "extractor.export_to_csv(output_dir=\"./extracted_csv_tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544c1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdm23_env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
